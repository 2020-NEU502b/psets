{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDP Problem Set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=1.25)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: MNIST Dataset\n",
    "The [MNSIT dataset](https://en.wikipedia.org/wiki/MNIST_database) is a large database of handwritten digits. Each digit has been preprocessed to be in black-and-white and to fit into a 28x28 pixel bounding box. The challenge of the MNIST dataset is to train a machine learning model that can accurately classify the digit class (0-9) from a given image. Some example images from the MNIST dataset are presented below:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" />\n",
    "\n",
    "\n",
    "For the sake of this exercise, we will only use 1000 example images per digit. We load the data below. All of the digit images are stored in **data**, whereas the categories are stored in **labels**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data from compressed file.\n",
    "npz = np.load('mnist.npz')\n",
    "\n",
    "## Extract data.\n",
    "data, labels = npz['data'], npz['labels']\n",
    "\n",
    "print('data:   shape = (%s, %s)' %data.shape)\n",
    "print('labels: shape = (%s,)' %labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm the accuracy of the labels, plot a composite image of each digit 0-9. In other words, plot an average of each digit using your favorite heatmap function.\n",
    "\n",
    "**Note:** To plot the 2d image, you will need to reshape the second axis from *shape=(784)* to *shape=(28,28)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Logistic (Multinomial) Classification\n",
    "\n",
    "In this next section, we will predict the category label using a [multinomial classifier](https://en.wikipedia.org/wiki/Multinomial_distribution). Multinomial classification is just an extension of logistic regression to handle more than 2 categories. As such, multinomial classification can only model linear relationships.\n",
    "\n",
    "Import the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) classifier from *scikit-learn* and fit it to the dataset using the following parameters:\n",
    "\n",
    "- C = 0.01\n",
    "- multi_class='multinomial'\n",
    "- solver='lbfgs'\n",
    "- max_iter=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the classification weights from `fit.coef_`. Plot the classification weights for each of the ten digits, just as you did for the average digit images above, and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Describe the classification weights for each digit. Are the weights isomorphic to their respective digits?\n",
    "\n",
    "> &nbsp;\n",
    "\n",
    "**Q:** What does this tell us about the \"representations\" returned by the multinomial classifier? Is it \"learning\" something new about the digits?\n",
    "\n",
    "> &nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Simple Neural Networks\n",
    "\n",
    "In this next section, we will predict the category label using a [multilayer perceptron](https://scikit-learn.org/stable/modules/neural_networks_supervised.html) (i.e. neural network). With neural networks, nonlinear relationships can be learned. \n",
    "\n",
    "Import the [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) classifier from *scikit-learn* and fit it to the dataset using the following parameters:\n",
    "\n",
    "- hidden_layer_sizes = (5,)\n",
    "- activation='logistic'\n",
    "- solver='sgd' \n",
    "- alpha=1 \n",
    "- learning_rate='constant'\n",
    "- learning_rate_init=0.1,\n",
    "- max_iter=1000\n",
    "- n_iter_no_change=100\n",
    "- momentum=0\n",
    "- random_state=0\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the weights of the first layer from `fit.coef_`. Plot the weights for each of the five hidden units and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Describe the classification weights for each digit. Are the weights isomorphic to their respective digits?\n",
    "\n",
    "> &nbsp;\n",
    "\n",
    "**Q:** What does this tell us about the \"representations\" returned by the neural network? Is it \"learning\" something new about the digits?\n",
    "\n",
    "> &nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the weights from the second layer (again using a heatmap) with the 5 hidden units on the x-axis and the 10 outputs on the y-axis. Answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Based on what you know about the architecture of neural networks, what can we infer from the second layer of weights? In other words, what do the weights of the second layer tell us about the relationship between the \"latent representations\" learned in the first layer and the output labels?\n",
    "\n",
    "> &nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting way to demonstrate the relationship between the two weight layers is to run the network \"in reverse\". In other words, we can pretend that we passed a digit *label* to the output layer of the network and observe what  28x28 image it predicts. \n",
    "\n",
    "A \"hacky\" way to run the network in reverse is to take the dot-product of the two weight layers. (We say this is hacky because we are ignoring the activation functions and bias units. That is ok for this simple exercise.) Below, compute the dot product between the two layers. You will end up with 10 \"predicted\" images. Plot them and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Do the images resemble the original digits? (The images may be clearer if you normalize each image to be on the same scale. Because we are using a \"hacky\" approach, do not worry if they are not perfect.)\n",
    "\n",
    "> &nbsp;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
